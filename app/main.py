import streamlit as st
import os
import sys

# Add project root to sys.path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from app.graph_loader import load_graph, get_schema_info
from app.core_logic import generate_sparql, execute_sparql, generate_answer
import google.generativeai as genai

# Page Config
st.set_page_config(
    page_title="SNU í•™ì‹ ì§€ì‹ ê·¸ë˜í”„",
    page_icon="ğŸ²",
    layout="wide"
)

# Constants
ABOX_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), '../data/knowledge_graph/abox_inferred.ttl'))

# Session State Initialization
if "messages" not in st.session_state:
    st.session_state.messages = []

if "graph" not in st.session_state:
    with st.spinner("ì§€ì‹ ê·¸ë˜í”„ë¥¼ ë¡œë”© ì¤‘ì…ë‹ˆë‹¤... (ì•½ 10ì´ˆ ì†Œìš”)"):
        try:
            st.session_state.graph = load_graph(ABOX_PATH)
            st.session_state.schema_info = get_schema_info(st.session_state.graph)
            st.success("ì§€ì‹ ê·¸ë˜í”„ ë¡œë“œ ì™„ë£Œ!")
        except Exception as e:
            st.error(f"ê·¸ë˜í”„ ë¡œë“œ ì‹¤íŒ¨: {e}")
            st.stop()

def get_explanation(query: str) -> str:
    """
    Generates an explanation for the SPARQL query using Gemini.
    """
    try:
        model = genai.GenerativeModel('gemini-3-pro-preview')
        prompt = f"""
        Explain the following SPARQL query in simple Korean.
        Focus on what criteria are used to filter the data.
        
        [SPARQL Query]
        {query}
        """
        response = model.generate_content(prompt)
        return response.text
    except:
        return "ì¿¼ë¦¬ í•´ì„ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

# Title & Sidebar
st.title("ğŸ“ SNU Dining Knowledge Graph")
st.markdown("ì„œìš¸ëŒ€í•™êµ í•™ì‹ ë©”ë‰´ë¥¼ **ì˜¨í†¨ë¡œì§€ ê¸°ë°˜**ìœ¼ë¡œ ìŠ¤ë§ˆíŠ¸í•˜ê²Œ ê²€ìƒ‰í•´ë³´ì„¸ìš”.")

with st.sidebar:
    st.header("ì‚¬ìš© ê°€ì´ë“œ")
    st.markdown("""
    ì´ ì„œë¹„ìŠ¤ëŠ” **Graph RAG (Retrieval Augmented Generation)** ê¸°ìˆ ì„ í™œìš©í•©ë‹ˆë‹¤.
    
    1. **ì§ˆë¬¸ ì…ë ¥**: ì°¾ê³  ì‹¶ì€ ë©”ë‰´ë‚˜ ì‹ë‹¹ ì¡°ê±´ì„ ìì—°ì–´ë¡œ ì§ˆë¬¸í•˜ì„¸ìš”.
    2. **íˆ¬ëª…í•œ ì¶”ë¡ **: AIê°€ ì–´ë–»ê²Œ SPARQL ì¿¼ë¦¬ë¥¼ ì§œê³  ë‹µì„ ì°¾ì•˜ëŠ”ì§€ ë³´ì—¬ì¤ë‹ˆë‹¤.
    
    **ğŸ’¡ ì¶”ì²œ ì§ˆë¬¸**
    - "ì˜¤ëŠ˜ ì ì‹¬ì— 5000ì› ì´í•˜ ë©”ë‰´ ìˆì–´?"
    - "ê³µëŒ€ ê·¼ì²˜ì—ì„œ ë©´ ìš”ë¦¬ íŒŒëŠ” ê³³ ì•Œë ¤ì¤˜"
    - "ì €ë…ì— ìš´ì˜í•˜ëŠ” ì‹ë‹¹ ì–´ë””ì•¼?"
    - "ëˆê¹ŒìŠ¤ íŒŒëŠ” ê³³ ì°¾ì•„ì¤˜"
    """)
    st.divider()
    st.caption(f"Graph Triples: {len(st.session_state.graph):,}")

# Chat Interface
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])
        if "details" in message:
            with st.expander("ğŸ” ì¶”ë¡  ê³¼ì • ë° ê·¼ê±° ë°ì´í„° (í´ë¦­í•´ì„œ í¼ì¹˜ê¸°)"):
                st.markdown("**[1ë‹¨ê³„] SPARQL ì¿¼ë¦¬**")
                st.code(message["details"]["sparql"], language="sparql")
                
                st.markdown("**[2ë‹¨ê³„] ì¿¼ë¦¬ í•´ì„**")
                st.write(message["details"]["explanation"])
                
                st.markdown("**[3ë‹¨ê³„] ê·¼ê±° ë°ì´í„° (Raw Data)**")
                if message["details"]["raw_data"]:
                    st.dataframe(message["details"]["raw_data"])
                else:
                    st.info("ì¡°ê±´ì— ë§ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    # User Message
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

    # Assistant Response
    with st.chat_message("assistant"):
        with st.status("ì§€ì‹ ê·¸ë˜í”„ì—ì„œ ë‹µì„ ì°¾ëŠ” ì¤‘...", expanded=True) as status:
            
            # Step 1: SPARQL Generation
            status.write("ğŸ§  1. ì§ˆë¬¸ ì´í•´ ë° SPARQL ì¿¼ë¦¬ ì‘ì„± ì¤‘...")
            sparql_query = generate_sparql(prompt, st.session_state.schema_info)
            if not sparql_query:
                st.error("SPARQL ì¿¼ë¦¬ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                st.stop()
            
            # Step 2: Query Execution
            status.write("ğŸ” 2. ì§€ì‹ ê·¸ë˜í”„ ê²€ìƒ‰ (SPARQL ì‹¤í–‰)...")
            raw_data = execute_sparql(sparql_query, st.session_state.graph)
            
            # Step 3: Explanation & Answer Generation
            status.write("ğŸ“ 3. ê²°ê³¼ í•´ì„ ë° ë‹µë³€ ì‘ì„± ì¤‘...")
            explanation = get_explanation(sparql_query)
            final_answer = generate_answer(prompt, raw_data, sparql_query)
            
            status.update(label="ë‹µë³€ ìƒì„± ì™„ë£Œ!", state="complete", expanded=False)

        # Output
        st.write(final_answer)
        
        # Save context for history with details
        st.session_state.messages.append({
            "role": "assistant", 
            "content": final_answer,
            "details": {
                "sparql": sparql_query,
                "explanation": explanation,
                "raw_data": raw_data
            }
        })
        
        # Show details immediately for the current turn
        with st.expander("ğŸ” ì¶”ë¡  ê³¼ì • ë° ê·¼ê±° ë°ì´í„° (í´ë¦­í•´ì„œ í¼ì¹˜ê¸°)", expanded=True):
            st.markdown("**[1ë‹¨ê³„] SPARQL ì¿¼ë¦¬**")
            st.code(sparql_query, language="sparql")
            
            st.markdown("**[2ë‹¨ê³„] ì¿¼ë¦¬ í•´ì„**")
            st.write(explanation)
            
            st.markdown("**[3ë‹¨ê³„] ê·¼ê±° ë°ì´í„° (Raw Data)**")
            if raw_data:
                st.dataframe(raw_data)
            else:
                st.info("ì¡°ê±´ì— ë§ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
